from keras.models import Sequential
from keras.layers import Dense
from Util import *
import keras.backend as K
from sklearn import metrics
import seaborn as sns
import tensorflow as tf

feature_vector_length = 5
num_classes = 10
def load_dataset():
    # load dataset
    encoded_train_imgs = np.load('encoded_train_imgs_5.npy')
    train_Y = np.load('train_labels_5.npy')
    encoded_test_imgs = np.load('encoded_test_imgs_5.npy')
    test_Y = np.load('test_labels_5.npy')

    # encoded_train_imgs = np.expand_dims(encoded_train_imgs, axis=2)
    # encoded_test_imgs = np.expand_dims(encoded_test_imgs, axis=2)

    print("Training set (images) shape: {shape}".format(shape=encoded_train_imgs.shape))
    print("Test set (images) shape: {shape}".format(shape=encoded_test_imgs.shape))
    print("Training set (labels) shape: {shape}".format(shape=train_Y.shape))
    print("Test set (labels) shape: {shape}".format(shape=test_Y.shape))
    return encoded_train_imgs, train_Y, encoded_test_imgs, test_Y


encoded_train_imgs, train_Y, encoded_test_imgs, test_Y = load_dataset()
input_shape = (feature_vector_length,)

def nll(y_true, y_pred):
    """ Negative log likelihood. """
    y_pred_mean = K.mean(y_pred)
    y_pred_std = K.std(y_pred)
    square = K.square(y_pred_mean-y_true)
    mse = -0.5*K.sum((square/y_pred_std))
    sigma_trace = -K.sum(K.log(y_pred_std))
    loss = mse + sigma_trace
    return K.mean(-loss)

def nll2(y_true, y_pred):
    n, p = tf.unstack(y_pred, num=10, axis=1)
    n = tf.expand_dims(n, -1)
    p = tf.expand_dims(p, -1)
    nll = (
        tf.math.lgamma(n)
        + tf.math.lgamma(y_true + 1)
        - tf.math.lgamma(n + y_true)
        - n * tf.math.log(p)
        - y_true * tf.math.log(1 - p)
    )
    return nll

def nll3(y_true, y_pred):
    n_dims = int(int(y_pred.shape[1]) / 2)
    mu = y_pred[:, 0:n_dims]
    logsigma = y_pred[:, n_dims:]

    mse = -0.5 * K.sum(K.square((y_true - mu) / K.exp(logsigma)), axis=1)
    sigma_trace = -K.sum(logsigma, axis=1)
    log2pi = -0.5 * n_dims * np.log(2 * np.pi)

    log_likelihood = mse + sigma_trace + log2pi

    return K.mean(-log_likelihood)

# Create the model
model = Sequential()
model.add(Dense(350, input_shape=input_shape, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(num_classes))

# Configure the model and start training
model.compile(loss=nll3, optimizer='adam', metrics=['accuracy'])
history = model.fit(encoded_train_imgs, train_Y, epochs=10, batch_size=100, verbose=1, validation_split=0.2)

# predict
y_pred = model.predict(encoded_test_imgs)
Y_pred = np.argmax(y_pred, 1) # Decode Predicted labels
Y_test = np.argmax(test_Y, 1) # Decode labels
# accuracy
print("accuracy:", metrics.accuracy_score(y_true=Y_test, y_pred=Y_pred), "\n")
# cm
mat = metrics.confusion_matrix(y_true=Y_test, y_pred=Y_pred)
print(mat)
sns.heatmap(mat.T, square=True, annot=True, cbar=False, cmap=plt.cm.Blues)
plt.xlabel('Predicted Values')
plt.ylabel('True Values')
plt.show()

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()